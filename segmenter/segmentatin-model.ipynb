{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Model\n",
    "\n",
    "Here I intend to develop a segmentation model with the aid of previously pre-trained deep neural models. To load the GloVe Embeddings, I followed [this tutorial](https://keras.io/examples/nlp/pretrained_word_embeddings/).\n",
    "\n",
    "---\n",
    "### To Do:\n",
    "- Find a way to best embed numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 17:52:10.613515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/oracle/instantclient_19_11:\n",
      "2021-12-29 17:52:10.613594: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MIN_TOKENS = 30\n",
    "MAX_TOKENS = 200\n",
    "VOCAB_SIZE = 20000\n",
    "VALIDATION_PCT = 0.15\n",
    "TEST_PCT = 0.15\n",
    "\n",
    "def get_sentence_labels(sentence):\n",
    "    \"\"\"\n",
    "        This function gets the label for each word on the sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    ORDINARY_TOKEN = 0\n",
    "    END_OF_SENTENCE_TOKEN = 1\n",
    "    \n",
    "    labels = []\n",
    "    for i, sentence in enumerate(sentences, start=1):\n",
    "        n_words = len(sentence.split())\n",
    "\n",
    "        if (sentence == ''):\n",
    "            continue\n",
    "\n",
    "        partial_labels = [ORDINARY_TOKEN] * n_words\n",
    "\n",
    "        if (i != len(sentences)):\n",
    "            partial_labels[-1] = END_OF_SENTENCE_TOKEN\n",
    "\n",
    "        labels += partial_labels\n",
    "    \n",
    "    return labels\n",
    "    \n",
    "\n",
    "def build_random_sized_sentences(text):\n",
    "    \"\"\"\n",
    "        Build random sized sentences out of a single corpus per function call\n",
    "    \"\"\"\n",
    "    global MIN_TOKENS\n",
    "    global MAX_TOKENS\n",
    "    \n",
    "    text_tokens = text.split()\n",
    "    prev_index = 0\n",
    "    sentences = []\n",
    "    while prev_index < len(text_tokens):\n",
    "        final_index = prev_index + np.random.randint(MIN_TOKENS, MAX_TOKENS)\n",
    "        sentences.append(' '.join(text_tokens[prev_index:final_index]))\n",
    "        \n",
    "        prev_index = final_index\n",
    "        \n",
    "    return np.array(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training sentences\n",
    "\n",
    "The training sentences are built by drawing random sentence sizes, and sequentially slicing the original corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this is the air jordan 3 black cement. this mi...</td>\n",
       "      <td>https://www.ted.com/talks/josh_luber_why_sneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if you want to buy high quality low price coca...</td>\n",
       "      <td>https://www.ted.com/talks/jamie_bartlett_how_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>do you know how many choices you make in a typ...</td>\n",
       "      <td>https://www.ted.com/talks/sheena_iyengar_how_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           subtitle  \\\n",
       "0           0  this is the air jordan 3 black cement. this mi...   \n",
       "1           1  if you want to buy high quality low price coca...   \n",
       "2           2  do you know how many choices you make in a typ...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/josh_luber_why_sneak...  \n",
       "1  https://www.ted.com/talks/jamie_bartlett_how_t...  \n",
       "2  https://www.ted.com/talks/sheena_iyengar_how_t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/TED_Talks/02_preprocessed/subtitles_preprocessed.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_corpora = df.subtitle.apply(lambda text: build_random_sized_sentences(text))\n",
    "entire_corpora = np.concatenate(entire_corpora.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(entire_corpora)\n",
    "train_upper_idx = int(corpus_size * (1 - VALIDATION_PCT - TEST_PCT))\n",
    "valid_upper_idx = int(corpus_size * (1 - TEST_PCT))\n",
    "\n",
    "train_data = entire_corpora[:train_upper_idx]\n",
    "valid_data = entire_corpora[train_upper_idx:valid_upper_idx]\n",
    "test_data = entire_corpora[valid_upper_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 17:52:21.929487: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-12-29 17:52:21.929597: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kunumi): /proc/driver/nvidia/version does not exist\n",
      "2021-12-29 17:52:21.937839: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_TOKENS)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the most frequent words on the `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a mapper from words to their respective indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_index = {word: i for i, word in enumerate(vectorizer.get_vocabulary())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(f\"../data/embeddings/glove/glove.6B.{EMBEDDING_DIM}d.txt\") as file:\n",
    "    for line in file:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        embeddings[word] = np.fromstring(coefs, 'f', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 11195 words (265 misses)\n"
     ]
    }
   ],
   "source": [
    "hits, misses = 0, 0\n",
    "\n",
    "# +2 due to empty string and UKN token\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE+2, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        VOCAB_SIZE+2,\n",
    "        EMBEDDING_DIM,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(\n",
    "            units=20\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 40)               19360     \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,019,560\n",
      "Trainable params: 19,360\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
