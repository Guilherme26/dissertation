{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation as a filling task  (Masked Language Modeling)\n",
    "https://huggingface.co/transformers/usage.html#language-modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Dowloads and Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 02:23:19.386443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-24 02:23:19.386463: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/luiznery/Documents/locus/filmes/dissertation/env/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:694: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tatoeba Data\n",
    "https://tatoeba.org/pt-br/\n",
    "\n",
    "1. Dowload data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17511/2081131834.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[3] = df[2].str.replace('[^\\w\\s\\']',\"\").str.lower()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/sentences.csv',sep='\\t',header=None,)\n",
    "df = df[df[1]=='eng']\n",
    "df[3] = df[2].str.replace('[^\\w\\s\\']',\"\").str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(sentence, model, tokenizer, k=5):\n",
    "    \n",
    "    input = tokenizer.encode(sentence, return_tensors=\"pt\",padding=True, truncation=True,max_length=500, add_special_tokens = True)\n",
    "    mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    token_logits = model(input)[0]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "    top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n",
    "\n",
    "    return top_k_tokens\n",
    "        \n",
    "def tokens_to_strings(tokens):\n",
    "    return [tokenizer.decode([token]) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_punctuation(i, sent):\n",
    "    part1 = sent.split(' ')[:i+1]\n",
    "    part2 = sent.split(' ')[i+1:]\n",
    "    part1[-1] = part1[-1]+'.'\n",
    "    sent = \" \".join(part1+part2)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_segmentation(sent, model, tokenizer, k=5, puncts = ['.',',','-','!','?',':',';'], logic='any_punct' ):\n",
    "    sent = f\"{sent}\"\n",
    "    \n",
    "    if len(sent.replace(' ',''))==0:\n",
    "        return []\n",
    "    \n",
    "    if logic == 'any_punct': # if there is any punct token in the predicted tokens\n",
    "        for i in tqdm(range(len(sent.split(' ')))):\n",
    "            h = f\" \".join(sent.split(' ')[:i+1] + [f\"{tokenizer.mask_token}\"] + sent.split(' ')[i+1:])\n",
    "            possible_tokens = tokens_to_strings(get_top_k_tokens(h,model, tokenizer, k))\n",
    "\n",
    "            for p in puncts:\n",
    "                if p in possible_tokens:\n",
    "                    sent = perform_punctuation(i,sent)\n",
    "                    break\n",
    "        segments = [s for s in sent.split('.') if len(s)>0]\n",
    "        segments = [s if s[0] != ' ' else s[1:] for s in segments]\n",
    "        return segments\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 22.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"let's try something\",\n",
       " 'i have to go to sleep',\n",
       " 'today is june 18th',\n",
       " \"and it is muiriel's birthday\",\n",
       " 'muiriel is 20',\n",
       " 'now',\n",
       " 'the password is',\n",
       " 'muiriel']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \" \".join(df[3].str.lower()[:5])\n",
    "\n",
    "perform_segmentation(sent,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Segmenting our data - Subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles = pd.DataFrame()\n",
    "for file in os.listdir(\"../data/00_raw/\"):\n",
    "    subtitles = subtitles.append({\n",
    "        'id': file,\n",
    "        'text_raw': open('../data/00_raw/'+file).read()\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17511/826475033.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace(\"[^\\w\\s\\']+\",'')\n"
     ]
    }
   ],
   "source": [
    "text = subtitles.text_raw\n",
    "text = text.str.lower()\n",
    "text = text.str.replace('\\n','')\n",
    "text = text.str.replace(\"[^\\w\\s\\']+\",'')\n",
    "while text.str.contains('  ').any():\n",
    "    text = text.str.replace(\"  \",' ')\n",
    "subtitles['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our sentence: \n",
      "hey oh hey what's up bradley just want to take another look at you all right music wow i really need to shave hey guys carsten rehnquist year if you've been to any movie in the last few months you've  [...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 203/203 [00:55<00:00,  3.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"hey oh hey what's up\",\n",
       " 'bradley just want to take another look at you all right',\n",
       " 'music',\n",
       " 'wow',\n",
       " 'i really need to shave',\n",
       " 'hey guys',\n",
       " \"carsten rehnquist year if you've been to any movie in the last few months\",\n",
       " \"you've probably seen the trailer for a little movie called a star is born\",\n",
       " 'this film was been getting a lot of buzz and it stars lady',\n",
       " 'gaga and bradley cooper',\n",
       " 'who i believe also directed',\n",
       " \"it it's a remake of the original 1977 film\",\n",
       " \"and it's been getting a lot of buzz like i just said\",\n",
       " 'so',\n",
       " 'of course',\n",
       " 'i had to see it now based on the trailer',\n",
       " \"and the poster this really didn't seem like\",\n",
       " 'my type of thing',\n",
       " 'it seemed more like a film for people that live in the suburbs',\n",
       " 'who also have waller above their leather couches',\n",
       " 'that says live laugh',\n",
       " 'love and thing',\n",
       " 'is this movie',\n",
       " 'is not at all like that',\n",
       " \"it's not at all what\",\n",
       " 'it was promoted',\n",
       " 'as in my opinion',\n",
       " 'it seems like a story',\n",
       " \"that's all about this woman's rise\",\n",
       " 'to fame',\n",
       " 'but',\n",
       " 'at',\n",
       " 'least',\n",
       " 'to me',\n",
       " 'it was more about this',\n",
       " 'old country stars',\n",
       " 'battle with addiction',\n",
       " 'and alcoholism',\n",
       " \"it's actu\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Our sentence: \")\n",
    "print(subtitles.iloc[0]['text'][:200], '[...]')\n",
    "segments = perform_segmentation(subtitles.iloc[0]['text'][:1000],model,tokenizer)\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observações\n",
    "\n",
    "### Limitações\n",
    "\n",
    "1. Demora muito\n",
    "2. Limitação no numero maximo de tokens\n",
    "3. Ao pontuar, não considera nunca mais mudar aquele ponto\n",
    "\n",
    "### Notas\n",
    "\n",
    "- É prciso remover tokens de musica e etc das legendas do yt\n",
    "- Talvez valha a pena não considerar a virgula\n",
    "- Como medir se é bom?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
